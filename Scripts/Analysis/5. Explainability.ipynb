{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%run \"4. Model Selection and Evaluation.ipynb\"\n",
    "print('Operation complete')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use of SHAP and LIME for XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#come spieghiamo perchè il nostro modello ha preso questo numero?\n",
    "\n",
    "#in python ci sono 2 librerie di ti aiutano a costruire un explainer: lime e shap\n",
    "#entrambe molto utili nel mondo del lavoro\n",
    "\n",
    "from lime import lime_tabular\n",
    "\n",
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "            training_data=X_train, \n",
    "            feature_names=X_names, \n",
    "            class_names=\"Y\", \n",
    "            mode=\"regression\") #features, target e use case che stiamo facendo\n",
    "\n",
    "explained = explainer.explain_instance(X_test[0], \n",
    "                                       model.predict, \n",
    "                                       num_features=3) #andiamo a prevedere la prima oss del test set\n",
    "explained.as_pyplot_figure()\n",
    "plt.show()\n",
    "\n",
    "#lime ci spiega che la variabile che di più ha influito sulla predizione sono i metri quadri --> questa è una casa grane e\n",
    "#ciò ha portato il modello a prevedere un prezzo alto, cosi come hanno giocato al ribasso il fatto che la qualità è bassa\n",
    "# e che è anche una casa vecchia"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
