{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Web scraping script**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is the data used in a sentiment analysis usually obtained?**\n",
    "\n",
    " This script presents the web scraping part that consist in a technique used to extract data from websites and store it in a json file: I used OOP the create a class that can scrape Amazon reviews based on the ASIN (Amazon's product ID) given in input.\n",
    "\n",
    "---\n",
    "\n",
    "In this case I will scrape from Amazon the reviews on a product which is a pair of speakers.\n",
    "\n",
    "Reviews URL: 'https://www.amazon.co.uk/product-reviews/B075QVMBT9/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=1'\n",
    "\n",
    "---\n",
    "#### Steps:\n",
    "- Get the URL of the page to be scrapped\n",
    "- Inspect the elements of the page and identify the tags required\n",
    "- Access the URL\n",
    "- Get the element from the required tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the user-agent is private, I will store my personal user agent in a environment variable and recall it through the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "USER_AGENT = os.getenv(\"USER-AGENT\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class explanation:\n",
    "- **what is asin?** For amazon, the ASIN (Amazon Standard Identification Number) is basically the product identifier.\n",
    "- **User agents:** When scraping a website, you also need to set user-agents on every request as otherwise the website may block your requests because it knows you aren't a real user. This user agent will clearly identify your requests as coming from a web scraper, so the website can easily block you from scraping the site. User-agent found on by googling 'my user agent'.\n",
    "- **URL:** For the self.url variable i replaced the asin with the self.asin variable to make this script available for all products (given the asin of it). Note also that in the last part of the URL I left the 'pageNumber=' make it easier to iterate over all the reviews pages, that have different URLs.\n",
    "---\n",
    "\n",
    "For the **pagination** class method we use the session object created before to get the new url with the new page (obtained by concatenating the 2 strings self.url and the page number). The if means the following: if we don't find any reviews on the URL, return False (so when using this method in a loop, if its false we can break out the loop), otherwise return the response. This is going ot be really useful when lopping through many pages without knowing how many pages there are.\n",
    "\n",
    "---\n",
    "\n",
    "The **parse** method pulls out all the needed info, puts everything into a dictionary and that append it to a list which will return a list of many dictionaries which basically are all the reviews with their titles, star ratings and content. When runnign the code I incurred in many different errors, so i decided to put a try/except to handle these problems.\n",
    "\n",
    "---\n",
    "\n",
    "Lastly, the **save** method allows me to save the results in a json file.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** For selecting the needed pieces of the 'div' tag i used the CSS selector syntax for simplicity (https://requests.readthedocs.io/projects/requests-html/en/latest/) thanks to the requests_html library. This allows me to use the .find() method in a much more cleaner way rather than putting the class pf the 'div' element which is usually long and more complex, so for example:\n",
    "- a[target=value] where 'a' is the tag in which i am 'navigating' and in the brackets we have some class and its value that i want to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reviews:\n",
    "    def __init__(self, asin) -> None:\n",
    "        self.session = HTMLSession()     # session object that is used continuously whenever this class instance is called\n",
    "        self.headers = {'User-Agent': USER_AGENT}\n",
    "        self.asin = asin\n",
    "        self.url = f'https://www.amazon.co.uk/product-reviews/{self.asin}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber='\n",
    "\n",
    "\n",
    "    def pagination(self, page):\n",
    "        response = self.session.get(self.url + str(page), headers=self.headers)     # getting the new page URLs\n",
    "        # check if there are reviews in the new url o not, using CSS selectors\n",
    "        if not response.html.find('div[data-hook=review]'):\n",
    "            return False\n",
    "        else:\n",
    "            return response.html.find('div[data-hook=review]')\n",
    "    \n",
    "\n",
    "    def parse(self, reviews):\n",
    "        total = []\n",
    "        # looping on reviews to parse the data i will extract and put in a .json file later on\n",
    "        for review in reviews:\n",
    "            # .find() outputs a list, so with first=True i say that i want the first element of that list\n",
    "            # .text to convert into text (string)\n",
    "            try:\n",
    "                title = review.find('a[data-hook=review-title] span', first=True).text\n",
    "                rating = review.find('i[data-hook=review-star-rating] span', first=True).text   \n",
    "                # span means i'm selecting the span value of the element i[data-hook=review-star-rating]\n",
    "                # -> basically here i'm getting the number of stars of the review\n",
    "                place_date = review.find('span[data-hook=\"review-date\"]', first=True).text\n",
    "                content = review.find('span[data-hook=review-body] span', first=True).text.replace('\\n', '.').strip()\n",
    "                # .replace('\\n') and .strip() to eliminate backslashes and clean the text\n",
    "            except:\n",
    "                continue        # for sure will end up with some missing pages or data to be cleaned\n",
    "\n",
    "            data = {'title': title,\n",
    "                    'rating': rating,\n",
    "                    'place and date': place_date,\n",
    "                    'body': content[:2000]}      # crop excessively long reviews\n",
    "            total.append(data)\n",
    "        return total\n",
    "    \n",
    "\n",
    "    def save(self, results):\n",
    "        # opening a new file named 'ASIN_ID_reviews.json', in which i will write ('w') all the results\n",
    "        # assigning the new .json file to a new variable called f\n",
    "        with open(self.asin + '_reviews.json', 'w') as f:\n",
    "            json.dump(results, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 1\n",
      "Getting page 2\n",
      "Getting page 3\n",
      "Getting page 4\n",
      "Getting page 5\n",
      "Getting page 6\n",
      "Getting page 7\n",
      "Getting page 8\n",
      "Getting page 9\n",
      "Getting page 10\n",
      "Getting page 11\n",
      "Getting page 12\n",
      "Getting page 13\n",
      "Getting page 14\n",
      "Getting page 15\n",
      "Getting page 16\n",
      "Getting page 17\n",
      "Getting page 18\n",
      "Getting page 19\n",
      "Getting page 20\n",
      "Getting page 21\n",
      "Getting page 22\n",
      "Getting page 23\n",
      "Getting page 24\n",
      "Getting page 25\n",
      "Getting page 26\n",
      "Getting page 27\n",
      "Getting page 28\n",
      "Getting page 29\n",
      "Getting page 30\n",
      "Getting page 31\n",
      "Getting page 32\n",
      "Getting page 33\n",
      "Getting page 34\n",
      "Getting page 35\n",
      "Getting page 36\n",
      "Getting page 37\n",
      "Getting page 38\n",
      "Getting page 39\n",
      "Getting page 40\n",
      "Getting page 41\n",
      "Getting page 42\n",
      "Getting page 43\n",
      "Getting page 44\n",
      "Getting page 45\n",
      "Getting page 46\n",
      "Getting page 47\n",
      "Getting page 48\n",
      "Getting page 49\n",
      "Getting page 50\n",
      "Getting page 51\n",
      "Getting page 52\n",
      "Getting page 53\n",
      "Getting page 54\n",
      "Getting page 55\n",
      "Getting page 56\n",
      "Getting page 57\n",
      "Getting page 58\n",
      "Getting page 59\n",
      "Getting page 60\n",
      "Getting page 61\n",
      "Getting page 62\n",
      "Getting page 63\n",
      "Getting page 64\n",
      "Getting page 65\n",
      "Getting page 66\n",
      "Getting page 67\n",
      "Getting page 68\n",
      "Getting page 69\n",
      "Getting page 70\n",
      "Getting page 71\n",
      "Getting page 72\n",
      "Getting page 73\n",
      "Getting page 74\n",
      "Getting page 75\n",
      "Getting page 76\n",
      "Getting page 77\n",
      "Getting page 78\n",
      "Getting page 79\n",
      "Getting page 80\n",
      "Getting page 81\n",
      "Getting page 82\n",
      "Getting page 83\n",
      "Getting page 84\n",
      "Getting page 85\n",
      "Getting page 86\n",
      "Getting page 87\n",
      "Getting page 88\n",
      "Getting page 89\n",
      "Getting page 90\n",
      "Getting page 91\n",
      "Getting page 92\n",
      "Getting page 93\n",
      "Getting page 94\n",
      "Getting page 95\n",
      "Getting page 96\n",
      "Getting page 97\n",
      "Getting page 98\n",
      "Getting page 99\n",
      "Getting page 100\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':              \n",
    "# This first line makes sure that this runs only when directly executed and not imported as a module\n",
    "amz = Reviews('B075QVMBT9')\n",
    "results = []\n",
    "for i in range(1,101):              # 101 => a url provides n reviews with 10 reviews on each page (first 1000 reviews)\n",
    "    print('Getting page', i)\n",
    "    time.sleep(0.3)                 # to let the loop \"breathe\"\n",
    "    reviews = amz.pagination(i)\n",
    "    # if there is something into reviews (seeing the many errors i've gotten) -> append it to the results\n",
    "    if reviews:                     \n",
    "        results.append(amz.parse(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the desired pages are scraped but there are for sure some pages that were blocked by Amazon: let's check all the non-scraped pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The non-scrapped pages are:  [75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "ind=[]\n",
    "for i in range(0,100):\n",
    "    if len(results[i]) == 0:\n",
    "        ind.append(i)\n",
    "print(\"The non-scrapped pages are: \", ind)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it ended up with a solid result: more than 730 reviews! Proceeding now to save the reviews in a json file which will be later converted a Pandas DataFrame to proceed with the sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "amz.save(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
